{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "DanielFernandez Tarea 2 2021.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantek6/Order-summary-component/blob/main/DanielFernandez_Tarea_2_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8yVz31amY0"
      },
      "source": [
        "## <img src=\"https://www.upla.cl/normasgraficas/wp-content/uploads/2016/01/logo_upla.png\" title=\"Title text\" width=\"35%\" height=\"20%\" />\n",
        "\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> CIF 8458 Ingeniería del Conocimiento II-2021 </h1>\n",
        "\n",
        "<H3 align='center'> Tarea 2 - Redes Neuronales y Aprendizaje No supervisado </H3>\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "**Temas**  \n",
        "* Redes Densas Feed Forward\n",
        "* Regularización y Dropout\n",
        "* Vanishing Gradient y Skip Connections\n",
        "* Learn Rate Decay\n",
        "* Optimizadores\n",
        "* Aprendizaje No Supervisado\n",
        "\n",
        "\n",
        "** Formalidades **  \n",
        "* Se debe realizar un jupyter notebook con los pasos, descripciones, análisis y conclusiones.\n",
        "* La tarea es individual, la copia parcial o total será penalizada con nota 1.\n",
        "* La entrega de la tarea se realizará en el sistema <em>eaula </em> (se debe adjuntar el archivo .ipynb).\n",
        "* Fecha de Entrega: Domingo 12 de Diciembre, 23:00 horas. \n",
        "* El sistema no aceptarán tareas con retraso.\n",
        "* Debe estar preparad@ para presentar su tarea en horario a determinar. \n",
        "* La Nota del informe se calcula: $\\left(\\frac{\\mbox{suma de puntos obtenidos}*6}{50}\\right)+1$\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "**Uso de google colab**\n",
        "Se recomienda el uso de google colab para evitar usar los recursos de su computador local.\n",
        "\n",
        "**Paquetes instalación**\n",
        "Para poder trabajar en el curso se necesitará instalar librerías para Python, por lo que se recomienda instalarlas a través de anaconda (para Windows y sistemas Unix) en un entorno virtual, donde podrán elegir su versión de Python. Se instalarán librerías como sklearn, una librería simple y de facil acceso para data science, keras en su versión con GPU (para cálculo acelerado a través de la tarjeta gráfica), además de que ésta utiliza como backend TensorFlow o Theano, por lo que habrá que instalar alguno de éstos, además de las librerías básicas de computer science como numpy, matplotlib, pandas, además de claramente jupyter.\n",
        "\n",
        "Descargar anaconda\n",
        "\n",
        "Luego de instalar Anaconda y tenerla en el path de su computador crear un entorno virtual:\n",
        "\n",
        "conda create -n redesneuronales python=version\n",
        "con version, la version de Python que desea utilizar. Si está en Windows, se recomienda Python 3 (es decir, python = 3) debido a dependencias con una de las librerías a utilizar.\n",
        "\n",
        "Acceder al ambiente creado\n",
        "\n",
        "source activate redesneuronales\n",
        "Instalar los paquetes a utilizar\n",
        "\n",
        "conda install jupyter sklearn numpy pandas matplotlib keras-gpu tensorflow-gpu\n",
        "(Si no tienen gpu instalan los paquetes keras y tensorflow en luegar de keras-gpu y tensorflow-gpu)\n",
        "\n",
        "Para salir del entorno\n",
        "source deactivate redesneuronales\n",
        "\n",
        "\n",
        "**Observaciones**\n",
        "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo, solo son guias y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes serán valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
        "Recuerden intercalar su código con *comentarios* en celdas _Markdown_, con los comentarios de la pregunta y con cualquier analisis, fórmula (en $ \\LaTeX $) o explicación que les parezca relevante para justificar sus procedimientos. *No respondan las preguntas en comentarios en el código*.\n",
        "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará tanto la elección en si. En cambio la argumentación detrás de la elección será lo más ponderado.\n",
        "Si algun modelo se demora demasiado en correr en su maquina, no olvide que puede correr _Jupyter Notebooks_ en _Collab_ de Google, incluso con la opción de aceleración con GPU (particularmente útil para los modelos más grandes), esto puede ser relevante para las maquinas más lentas al momento de realizar exploraciones con _K-folds_ o las redes más grandes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJd_5I3hamY5"
      },
      "source": [
        "# 1 - Redes Feed Forward para predecir la resistencia a la compresión del hormigón\n",
        "\n",
        "De las redes neuronales artificiales más simples se encuentran las redes densas o _Feed Forward_, donde todas las neuronas de una capa estan conectadas a todos los inputs y envian su señal de activación a todas las neuronas de la siguiente capa. Estas redes, si bien son las más simples, suelen tener desempeños bastante buenos, y en muchas aplicaciones reales son utilizadas, ya sea por si solas o en combinación con otros modelos. Además, son las redes donde más facil se pueden observar muchos de los fenómenos que se han descubierto a lo largo de los años de desarrollo de esta area del conocimiento, tanto por ser de las redes vigentes más antiguas y por su estructura relativamente simple. En esta primera parte de la tarea exploraremos las redes densas y algunos de sus hiperparámetros más relevantes como la profundidad, el número de unidades; estudiaremos también algunos métodos de regularización y evidenciaremos el problema del _vanishing gradient_ y el _exploding gradient_, viendo también algunos optimizadores existentes. \n",
        "\n",
        "Para realizar esto, utilizaremos una base de datos con información para determinar la resistencia a la compresión del hormigón (material muy importante para la construcciones, más para nosotros que vivimos en una zona sísmica), la cual se encuentra en el repositorio de datos de la Universidad de California, Irvine, en la siguiente URL: https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength. El dataset cuenta con 1030 registros, donde podemos encontrar la edad del hormigón y la información de sus ingredientes. \n",
        "Nuesta tarea durante esta pregunta será predecir la resistencia a la compresión del hormigón.\n",
        "\n",
        "<img src=\"https://civildigital.com/wp-content/uploads/2016/07/Compressive-Strength-test-for-M25-Concrete.jpg\" title=\"Title text\" width=\"35%\" height=\"20%\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CURAmEesamY6"
      },
      "source": [
        "### 1.a Carga de datos y primeros analisis\n",
        "(1 pto) Cargue los datos en un _dataframe_ como muestra el código. Explore superficialmente los datos utilizando los metodos `.head`, `.describe` o `.info` del _DataFrame_ y comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toHfTNSAamY7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_tr = pd.read_csv(\"Concrete_Data.xls\")\n",
        "\n",
        "# . . ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwxDQbRzamY8"
      },
      "source": [
        "### 1.b Estandarización y Train Test Split\n",
        "(2 pts) En esta pregunta nos ocuparemos de separar el _dataset_ en los conjuntos de entrenamiento, validación y test y estandarizar los datos. Para esto puede utilizar la librería sklearn, en particular las funciones `StandarScaler` y `train_test_split`.\n",
        "\n",
        "Para esto separe primero el dataset en $X$ e $Y$. Luego separe los datos considerando un $70\\%$ de ellos para entrenamiento, un $20\\%$ para validación y un $10\\%$ para test. Finalmente ajuste los _scalers_ con los datos de entrenamiento y transforme los datos. \n",
        "\n",
        "- ¿Qué operación matemática realiza `StandarScaler` al momento de tranformar los datos? \n",
        "- ¿Por qué debemos transformar los datos de validación y de test con el _scaler_ ajustado a los datos de entrenamiento? \n",
        "- ¿Qué estamos tratando de representar en esta separación en conjuntos de entrenamiento, validación y test?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGNbVubWamY9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "scaler_x = StandardScaler()\n",
        "scaler_x.fit(X_tr)\n",
        "x_tr = scaler_x.transform(X_tr)\n",
        "x_val = scaler_x.transform(X_val)\n",
        "\n",
        "# . . ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftVH16OFamY9"
      },
      "source": [
        "### 1.c Primera Red\n",
        "(6 pts) En esta pregunta construiremos y entrenaremos una primera red neuronal. Para esto utilizaremos la librería keras que se ocupa de crear, compilar y entrenar los modelos de manera simple. Keras se encargará por lo tanto de crear los modelos y al momento de compilarlos se instanciaran estos en una sesión de TensorFlow. \n",
        "\n",
        "Esta primera red será una red de una capa oculta con $256$ neuronas, activación ReLu. Para esta red y todas las demas utilizaremos la función de pérdida _Mean Square Error_ para obtener resultados comparables entre distintos modelos. Para entrenar esta red utilizaremos Gradiente Descendente Estocástico con un _Learn Rate_ de 0.002. Finalmente entrenaremos esta red por unas 20 _epochs_. \n",
        "\n",
        "* Construya la red basandose en el código y la documentación de keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxdgN30-amY-"
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "ANN = Sequential()\n",
        "\n",
        "# Hidden Layer\n",
        "ANN.add(\n",
        "    Dense(\n",
        "        units = # n units, \n",
        "        activation = 'relu'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Output Layer\n",
        "ANN.add(Dense\n",
        "       units = # dimension of Output... ,\n",
        "        # no need for activation (i.e. linear activation) considering the range of the output... \n",
        "       )\n",
        "\n",
        "ANN.compile(\n",
        "    optimizer=SGD(# . . .  , \n",
        "    loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUKb1AZamY-"
      },
      "source": [
        "Una forma fácil de instanciar la red es la propuesta en el codigo abajo, es decir entrenar la red por 0 _epochs_. Una red instanciada nos permite utilizar el método `.summary` para ver su número de parametros y los tamaños de cada capa. \n",
        "\n",
        "* Explique el número de parámetros presentes en esta red, es decir: ¿Cómo a partir de la dimensión del _Input_ y el número de neuronas obtenemos ese número de parámetros?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK2GfGAXamY_"
      },
      "source": [
        "ANN.fit(x_tr, y_tr, epochs=0)\n",
        "ANN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7YU1GWJamZA"
      },
      "source": [
        "* Entrene la red por 20 _epochs_, guardando el `history` que retorna el metodo `.fit`.\n",
        "\n",
        "* Grafique como varian los errores de validación y de entrenamiento a lo largo de las _epochs_. Comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwvETLpgamZA"
      },
      "source": [
        "history = model.fit(x_tr, y_tr, epochs=20, validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y8VkAvSamZA"
      },
      "source": [
        "* Cree y entrene nuevamente la red, esta vez cambiando el _learn rate_ utilizado para el SGD. Pruebe a lo menos dos valores mayores y dos valores menores al elegido anteriormente. Note que para valores mayores al propuesto puede comenzar a observar fenómeno de divergencia, por lo cual es recomendable agregarle a la red un _calback_, es decir una función que verifica estados y comportamientos de la red mientras se entrena, en particular `TerminateOnNaN`, el cual interrumpirá el proceso de entrenamiento si encuentra un valor NaN. \n",
        "\n",
        "* Grafique el comportamiento de los errores de validación y entrenamiento para a lo menos un valor mayor y uno menor al original y comente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4EhV7tIamZA"
      },
      "source": [
        "from keras.callbacks import TerminateOnNaN\n",
        "\n",
        "# . . . \n",
        "\n",
        "history = ANN.fit( # . . .\n",
        "         callbacks=[TerminateOnNaN]\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQwYyPqOamZB"
      },
      "source": [
        "### 1.d Activación y regularizadores $l$1 $l$2\n",
        "(4 pts) En esta pregunta se les propone explorar distintas funciones de activación o de regularización usual. Por lo extenso de la tarea se les propone elegir una de las dos exploraciones. En ambos casos deben entrenar la misma red entrenada anteriormente utilizando gradiente descendente con algun _learn rate_ que les parezca adecuado luego de la exploración en la pregunta anterior. \n",
        "\n",
        "* En caso de elegir explorar distintas funciones de activación, cambie la activación de la capa oculta sucesivamente por: tangente hiperbólica, _Leaky ReLu_, _softmax_, sigmoidea y lineal. Para esto puede basarse en el código presentado abajo y la documentación de keras. Para la activación _Leaky ReLu_ pruebe cambiar el parámetro de la red. Describa sus resultados y si observa diferencias entre las redes. \n",
        "\n",
        "* En caso de elegir explorar las funciones de regularización usual, agregue regularización $l$1 o $l$2 a la capa oculta y pruebe cambiar la tasa de regularización, reportando sus resultados. ¿Qué ocurre si la regularización es muy alta o muy baja? Una vez satisfecho con una tasa de regularización, aplique la regularización a la capa de salida y luego a ambas capas. \n",
        "\n",
        "**Independiente de la opción elegida**, comente sobre los siguientes temas:\n",
        "\n",
        "* ¿Cual es el interés de tener activaciones no lineales? ¿Le parece buena opción la activación sigmoidea para la capa oculta? ¿Qué pasaría si usaramos esta activación en la capa de salida? \n",
        "\n",
        "* ¿Cual es la intención de la regularización en general? En particular, ¿Que restricción implicita imponen las regularizaciones $l$1 o $l$2 sobre los pesos de la capa en la cual se aplican? Apoyese de ecuaciones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7QTvK52amZB"
      },
      "source": [
        "# activations\n",
        "from keras.layers import LeakyReLU\n",
        "model.add(Dense( # . . . ))\n",
        "model.add(LeakyReLU())\n",
        "    \n",
        "# regularizer\n",
        "from keras.regularizers import l1, l2\n",
        "model.add(\n",
        "    Dense( # . . .\n",
        "          activity_regularizer=l2(0.001)\n",
        "         )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIwei99amZB"
      },
      "source": [
        "### 1.e Exploración del Número de neuronas\n",
        "\n",
        "(3 pts) Ahora probaremos cambiando el número de neuronas en la capa oculta. Para esto, entrenen la red que estimen conveniente luego de la pregunta anterior, variando el numero de nuronas. Deben explorar a lo menos 10 número de neuronas distintos. Una recomendación sería por ejemplo explorar numero de neuronas en potencias de 2. \n",
        "\n",
        "Para cada red entrenada, recuperen el mejor error de validación y el error de entrenamiento en la _epoch_ donde se obtuvo tal error de validación. Grafique como se comportan ambos errores a medida crece el número de neuronas y comente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJROqraVamZC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_error, val_error = [], []\n",
        "\n",
        "for n_units in # . . . \n",
        "    \n",
        "    # do model \n",
        "    \n",
        "    # train model \n",
        "    \n",
        "    val_error = min(history.history['val_loss'])\n",
        "    train_error = history.history['loss'][np.argmin(history.history['val_loss'])] \n",
        "    # for instance\n",
        "    \n",
        "# . . . ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iOc7fPAamZC"
      },
      "source": [
        "### 1.f Dropout\n",
        "(3 pts) Como seguramente constataron en la pregunta anterior, un numero demasiado grande de parámetros en el modelo puede llevarnos a observar el fenomeno de _overfitting_. Una aproximación a este fenómeno que ha dado excelente resultado en redes neuronales es el método _dropout_, donde estocásticamente se desactivan una fracción de las neuronas al momento del entrenamiento, así efectivamente reduciendo el tamaño del modelo que se entrena en cada iteración e implicitamente obteniendo modelos más robustos por el simple hecho que al momento de entrenar nunca se entrena el \"mismo\" modelo. \n",
        "\n",
        "Según lo aprendido en el ramo, ¿en qué consiste el fenómeno de _overfitting_? ¿Por qué modelos más grandes suelen presentar el fenómeno? \n",
        "\n",
        "Entrene la mejor red obtenida en la pregunta anterior agregando una capa de _Dropout_ con parámetro $0.5$ inmediatamente luego de la capa oculta. Repita luego el proceso con una red con el doble de neuronas. Note que el agregar una capa _dropout_ hará que la red entrene más lento, por lo cual es recomendable aumentar el numero de _epochs_ para entrenar la red a completitud. \n",
        "\n",
        "¿Qué observa al agregar _dropout_? Comente y compare con sus resultados anteriores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5rNrcyvamZC"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# . . . \n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# . . ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSe8jzb1amZC"
      },
      "source": [
        "### 1.g Extreme Learning Machine\n",
        "\n",
        "(2 pts) Otra aproximación para obtener modelos grandes que no sobreajustan es la implementada por _ELM_. Explique en qué consiste la idea de _ELM_ y porqué esto podría evitar sobreajuste a pesar de utilizar modelos con gran número de parámetros. \n",
        "\n",
        "Entrene una _ELM_ de una capa fija y una capa oculta, la primera con un número relativamente grande y la segunda con un número relativamente pequeño. Puede utilizar los valores propuestos en el código u otros que le parezcan convenientes. \n",
        "\n",
        "Comente sobre el número total de parámetros y el número de parametros entrenables con respecto a los modelos anteriores. ¿Cómo se desempeña la red? ¿El número elevado de parámetros totales implica necesariamente _overfitting_?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj1h-PBIamZD"
      },
      "source": [
        "# . . . \n",
        "\n",
        "model.add(\n",
        "    Dense(units=5000,\n",
        "          activation=relu,\n",
        "         )\n",
        ")\n",
        "\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# . . . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6QRRiEPamZD"
      },
      "source": [
        "### 1.h Learning Rate Decay\n",
        "(3 pts) Ahora entrenaremos un modelo manejando manualmente el _learn rate_. Para esto utilizaremos el _callback_ `LearningRateScheduler`. Este _callback_ nos permitirá implementar una función que maneje el _learn rate_ de nuestro modelo. \n",
        "\n",
        "Escriba una función que reciba la epoca actual y retorne un _learn rate_ lr. El lr inicial debe ser igual o mayor a alguno que haya dado buenos resultados en las preguntas anteriores. La función debe dividir por 2 el lr cada 10 _epochs_. Además ponga como restricción que el lr no debe ser menor a $5\\times 10^{-5}$, es decir si el valor obtenido es menor a  $5\\times 10^{-5}$, la función retorna  $5\\times 10^{-5}$.\n",
        "\n",
        "Entrene su red preferida de las preguntas anteriores con esta modificación, grafique los errores a lo largo del entrenamiento y comente. Según lo visto en el ramo, ¿por qué podría ser util disminuir el _learn rate_ a medida se avanza en el aprendizaje de la red?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5sOk5boamZD"
      },
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def step_decay(epoch):\n",
        "    \n",
        "    \n",
        "    return lr\n",
        "\n",
        "schedule = LearningRateScheduler(step_decay)\n",
        "\n",
        "# model. # . . . \n",
        "\n",
        "model.fit(# . . .\n",
        "        callbacks=[schedule])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuMjV8pAamZD"
      },
      "source": [
        "### 1.i Vanishing Gradient\n",
        "\n",
        "(4 pts) El fenómeno del _vanishing gradient_ es el rápido decaimiento del paso de _Backpropagation_ al avanzar por las capas. A lo largo de la tarea solo hemos entrenado capas con una red oculta, de igual forma que la comunidad cientifica realizo por largo tiempo, por el problema del _vanishing gradient_ y por el teorema de aproximación universal que resumidamente demuestra que una red de una sola capa puede aproximar una amplia familia de funciones. \n",
        "\n",
        "En esta pregunta entrenaremos una red neuronal profunda sin implementar ninguno de los dispositivos que permiten hoy en día sortear el problema del _vanishing gradient_, para ponerlo en evidencia. Para esto construya una red con 6 capas ocultas, con la siguiente lista de numero de neuronas: $256$ $256$ $128$ $128$ $32$ y $32$, o con valores similares. De tal manera obtendrá un valor de parámetros relativamente comparable a los valores utilizados en las primeras redes. \n",
        "\n",
        "Grafique un histograma con los pesos de las 6 capas densas de la red sin entrenar, entrenela a completitud con el método que estime conveniente y luego grafique nuevamente los histogramas para las 6 capas. Comente lo que observa. \n",
        "\n",
        "Luego, pruebe cambiar la inizialización de los pesos de la capa densa, puede revisar la documentación de keras para ver las opciones existentes a parte de `glorot_uniform` por defecto. ¿Se logra solucionar el problema? \n",
        "\n",
        "Por último, pruebe aumentar la tasa de aprendizaje para ver si logra hacer que el paso de _backpropagation_ alcance las capas que anteriormente no se entrenaban. ¿Qué observa en este caso? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi8sgn_camZE"
      },
      "source": [
        "# . . .\n",
        "\n",
        "layer_kernel_weights = model.get_layer(index=i).get_weights()[0]\n",
        "layer_bias_weight = model.get_layer(index=i).get_weights()[1]\n",
        "# for one layer\n",
        "# you can also name your layers and call them by their names if it's less confusing\n",
        "\n",
        "# . . . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-tUTVZqamZE"
      },
      "source": [
        "### 1.j Otros Optimizadores\n",
        "\n",
        "(3 pts) Finalmente, utilizando la estructura de red que mejor se haya desempeñado a lo largo de la tarea, entrene esta red utilizando un optimizador distinto al gradiente descendente vainilla. Pruebe al menos 2 optimizadores implementados en keras (puede utilizar Adam, AdaGrad, AdaDelta, RMSprop, entre otros) o modificar los parámetros que no hemos utilizado del gradiente descendente (momentum, momentum de Nesterov...).\n",
        "\n",
        "Note que por las inicializaciones por defecto de los pesos de las capas y la naturaleza de los datos en cuestión, puede ocurrir que para los valores defecto de algunos optimizadores la red diverga en las primeras iteraciones. Para fijar los parámeros de los optimizadores debe importarlos desde `keras.optimizers` y pasar el objeto con los parámetros deseados al método `.compile` de su modelo. En cambio si con los valores usuales basta, algunos optimizadores pueden pasarse como `string` a `.compile`.\n",
        "\n",
        "Compare como se desempeñan estos optimizadores con la versión utilizada anteriormente, considerando los tiempos de entrenamiento, la velocidad de convergencia y el desempeño final alcanzado. Apoyese de gráficos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1OqNKFCamZE"
      },
      "source": [
        "# do it yourself"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j133VuFvamZE"
      },
      "source": [
        "### 1.k Testing \n",
        "\n",
        "(4 pts) Finalmente, luego de entrenar todos estos modelos estamos en condiciones de probar que tan bien fue nuestro desempeño. Para esto utilice el modelo en el cual obtuvo el mejor desempeño en validación y calcule el error cuadrático medio de la predicción realizada sobre el _Test set_. Para puede utilizar el metodo `.predict` de su modelo. \n",
        "\n",
        "¿Qué tan bien se desempeñaría su modelo en un caso real en vista de lo anterior? Si su curiosidad es suficiente, puede calcular el error real de su modelo transformando nuevamente el _target_ y su predicción a la escala original (utilizando su `scaler`) y tranformando a precio aplicando exponenciación (pues Y estaba espresado en escala logaritmica)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3RRcB8camZE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgvstVFLamZE"
      },
      "source": [
        " <a id=\"segundo\"></a>\n",
        "# 2. Clasificando granos de trigo usando Aprendizaje No Supervisado\n",
        "\n",
        "En esta sección utilizaremos el dataset Seeds que contiene información de mediciones de propiedades geométricas de granos de trigo pertenecientes a tres variedades: Kama, Rosa y Canadian. Para estudiar la estructura interna del grano se utilizó una técnica de rayos X de baja energía. Para ver en detalle la descripción de la semántica asociada a los atributos de este problema, puede consultar https://archive.ics.uci.edu/ml/datasets/seeds.\n",
        "\n",
        "\n",
        "<img src=\"https://sc04.alicdn.com/kf/Ud3ddc7f8ac0e48039a1a525543822bafU.jpg\" title=\"Title text\" width=\"40%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lClrPoojamZE"
      },
      "source": [
        "### 2.a) Carga de Datos\n",
        "\n",
        "(1 pto) Construya un dataframe con los datos a analizar. \n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "url = 'seeds_dataset.csv'\n",
        "df = pd.read_csv(url, sep=',',header=None, names=['AREA', 'PER', 'COMP', 'LEN', 'WID', 'ASYM', 'LGR','CLASS']) \n",
        "```\n",
        "\n",
        "Describa brevemente el dataset utilizar.\n",
        "\n",
        "```python\n",
        "df.info()\n",
        "df.describe()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t80qvc2hamZF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azMLWQOtamZF"
      },
      "source": [
        "### 2.b) Preprocesamiento\n",
        "\n",
        "(2 pts) Normalice los datos antes de trabajar. ¿Es necesario separar  los datos en entrenamiento y test?\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "targets = df.pop('CLASS')\n",
        "scaler = StandardScaler().fit_transform(df)\n",
        "df_scaled = pd.DataFrame(scaler, columns=df.columns)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reU6a1EVamZF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pB92vuBamZF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiIK6Mf9amZF"
      },
      "source": [
        "### 2.d) Etiquetados vía clustering\n",
        "\n",
        "(2 pts) Etiquete cada cluster con la etiqueta correspondiente. Explique que hace el siguiente código.\n",
        "\n",
        "```python\n",
        "from scipy.stats import mode\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(3):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(targets[mask])[0]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW1PRDo_amZF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Oyg_KIamZF"
      },
      "source": [
        "### 2.e) Cálculo del desempeño\n",
        "\n",
        "(2 pts) Calcule el accuracy de la clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JqtNF27amZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnL0KEfyamZG"
      },
      "source": [
        "### 2.f) Matriz de confusión\n",
        "\n",
        "(2 pts) Muestre la matriz de confusión. Comente\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "mat = confusion_matrix(targets, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, cbar=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68UGM5sZamZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijUmMtzuamZG"
      },
      "source": [
        "### 2.g) Mezcla de Gaussianas\n",
        "\n",
        "(2 pts) Clusterice los datos usando Mezcla de gaussianas, con el objetivo de clasificar el tipo de grano. Repita los pasos d) al g) usando este algoritmo. Compare con k-means y comente.\n",
        "\n",
        "```python\n",
        "from sklearn.mixture import GaussianMixture\n",
        "gmm = GaussianMixture(n_components=3).fit(df_scaled)\n",
        "clusters = gmm.predict(df_scaled)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZDHFgOramZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-6mAwRmamZG"
      },
      "source": [
        "### 2.h) Manifold\n",
        "\n",
        "(2 pts) Proyecte los datos usando el método de manifold TSNE. Compare k-means y mezcla de gaussianas usando esta proyección. ¿La proyección mejora los resultados?\n",
        "Comente.\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Project the data: this step will take several seconds\n",
        "tsne = TSNE(n_components=2, init='random', random_state=0)\n",
        "data_proj = tsne.fit_transform(df_scaled)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEIxBgNZamZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}